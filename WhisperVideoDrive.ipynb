{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/AndrewMayneProjects/Whisper/blob/main/WhisperVideoDrive.ipynb","timestamp":1678958987029}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"markdown","source":["If you're looking at this on GitHub and new to Python Notebooks or Colab, click the Google Colab badge above ðŸ‘†\n","\n","#ðŸ“¼ OpenAI Whisper + Google Drive Video Transcription\n","\n","ðŸ“º Getting started video: https://youtu.be/YGpYinji7II\n","\n","###This application will extract audio from all the video files in a Google Drive folder and create a high-quality transcription with OpenAI's Whisper automatic speech recognition system.\n","\n","*Note: This requires giving the application permission to connect to your drive. Only you will have access to the contents of your drive, but please read the warnings carefully.*\n","\n","This notebook application:\n","1. Connects to your Google Drive when you give it permission.\n","2. Creates a WhisperVideo folder and three subfolders (ProcessedVideo, AudioFiles and TextFiles.)\n","3. When you run the application it will search for all the video files (.mp4, .mov, mkv and .avi) in your WhisperVideo folder, transcribe them and then move the file to WhisperVideo/ProcessedVideo and save the transcripts to WhisperVideo/TextFiles. It will also add a copy of the new audio file to WhisperVideo/AudioFiles\n","\n","###**For faster performance set your runtime to \"GPU\"**\n","*Click on \"Runtime\" in the menu and click \"Change runtime type\". Select \"GPU\".*\n","\n","\n","**Note: If you add a new file after running this application you'll need to remount the drive in step 1 to make them searchable**"],"metadata":{"id":"oSzWV5We2jx0"}},{"cell_type":"code","source":[],"metadata":{"id":"TBOLg7X3mztt","executionInfo":{"status":"ok","timestamp":1678987136089,"user_tz":-60,"elapsed":584,"user":{"displayName":"Torsten Ek","userId":"13979891364184540453"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["##1. Load the code libraries"],"metadata":{"id":"pFx0mfr031aw"}},{"cell_type":"code","source":["!pip install git+https://github.com/openai/whisper.git \n","!pip install git+https://github.com/linto-ai/whisper-timestamped\n","!sudo apt update && sudo apt install ffmpeg\n","!pip install librosa\n","!pip install -Uqq ipdb\n","import ipdb\n","\n","import whisper\n","# import whisper_timestamped as whisper\n","import time\n","import librosa\n","import soundfile as sf\n","import re\n","import os\n","import json\n","\n","# model = whisper.load_model(\"tiny.en\")\n","# model = whisper.load_model(\"base.en\")  \n","model = whisper.load_model(\"small.en\") # load the small model\n","# model = whisper.load_model(\"medium.en\")\n","# model = whisper.load_model(\"large\")"],"metadata":{"id":"PomTPiCR5ihc","outputId":"50be39f4-794f-4e11-a521-b9d6e6f51cb9","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/openai/whisper.git\n","  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-meawgzcb\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-meawgzcb\n","  Resolved https://github.com/openai/whisper.git to commit 6dea21fd7f7253bfe450f1e2512a0fe47ee2d258\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (9.1.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (1.13.1+cu116)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (4.65.0)\n","Requirement already satisfied: tiktoken==0.3.1 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.3.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (1.22.4)\n","Requirement already satisfied: ffmpeg-python==0.2.0 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.2.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (2.0.0)\n","Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.56.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230314) (0.16.0)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2022.6.2)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2.28.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.9.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.22.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (15.0.7)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (63.4.3)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2.10)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (3.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2022.12.7)\n"]}]},{"cell_type":"markdown","source":["##2. Give the application permission to mount the drive and create the folders"],"metadata":{"id":"JIjETRxb5nuE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zxWvhDHzmspd"},"outputs":[],"source":["# Create the Drive folders\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True) # This will prompt for authorization.\n","\n","# This will create the WhisperVideo files if they don't exist.\n","folders =  [\"WhisperVideo/\", \"WhisperVideo/ProcessedVideo/\", \"WhisperVideo/TextFiles/\", \"WhisperVideo/AudioFiles/\", \"WhisperVideo/JsonFiles/\"]\n","for folder in folders:\n","  path = \"/content/drive/MyDrive/\" + folder\n","  if not os.path.exists(path): # Create the folder if it does not exist\n","    os.mkdir(path)\n"]},{"cell_type":"markdown","source":["## Add code to massage the result json into a json array of timestamped full senteces."],"metadata":{"id":"ZKN1tg0lycQw"}},{"cell_type":"code","source":["# Read the file data.json into a Python dictionary\n","\n","import json\n","\n","# Function to read a JSON file and return its content as a Python dictionary\n","def read_json_file(file_path):\n","    with open(file_path, 'r') as file:\n","        data = json.load(file)\n","    return data\n","\n","def find_end_of_sentence(segmentText):\n","    print(segmentText)\n","    # find the end of the sentence\n","    # if there is a period, then the end of the sentence is the period\n","    # if there is a question mark, then the end of the sentence is the question mark\n","    # if there is an exclamation point, then the end of the sentence is the exclamation point\n","    # if there is no period, question mark, or exclamation point, then the end of the sentence is the end of the string\n","    # return the end of the sentence\n","    end_of_sentence = -1\n","    if \".\" in segmentText:\n","        end_of_sentence = segmentText.find(\".\")\n","    elif \"?\" in segmentText:\n","        end_of_sentence = segmentText.find(\"?\")\n","    elif \"!\" in segmentText:\n","        end_of_sentence = segmentText.find(\"!\")\n","    return end_of_sentence\n","\n","# Function to determine if the segment is an incomplete sentence.\n","# If the segmentText does not contain a period, question mark, or exclamation point,\n","# then the segment is an incomplete sentence.\n","def is_incomplete_sentence(segmentText):\n","    end_of_sentence = find_end_of_sentence(segmentText)\n","    return True if end_of_sentence == -1 else False\n","\n","\n","# Transcribe traverses the segmentsArray starting from segmentIndex and forwards to find the\n","# end of the next complete sentence.\n","\n","def transcribe(accumulatedSentenceStart, accumulatedSentenceText, segmentIndex, segmentsArray):\n","    # We are at the end of the segmentsArray. Return.\n","    if segmentIndex >= len(segmentsArray):\n","        return\n","    currentSegment = segmentsArray[segmentIndex]\n","    currentSegmentText = currentSegment.get(\"text\")\n","    thisAccumulatedSentenceText = \"\"\n","    thisStartTimeStamp = currentSegment.get(\"start\")\n","\n","    # if previousSegment is None: then we set thisStartTimeStamp to the start time of the current segment\n","    # else we set thisStartTimeStamp to the end time of the previous segment\n","    if accumulatedSentenceStart is not None:\n","        thisStartTimeStamp = accumulatedSentenceStart\n","\n","    if accumulatedSentenceText is not None:\n","        thisAccumulatedSentenceText = accumulatedSentenceText\n","\n","    # If the segment is an incomplete sentence, then append the segmentText to the sentence\n","    # and call transcribe again with the next segment.\n","    if is_incomplete_sentence(currentSegmentText):\n","        newSentenceAccumulatedText = thisAccumulatedSentenceText + currentSegmentText\n","        transcribe(thisStartTimeStamp, newSentenceAccumulatedText,\n","                   segmentIndex + 1, segmentsArray)\n","    else:\n","        lastPeriodIndex = currentSegmentText.rfind(\".\")\n","        currentSentencesText = currentSegmentText[:lastPeriodIndex + 1]\n","        sentencesText = thisAccumulatedSentenceText + currentSentencesText\n","\n","        reminderText = currentSegmentText[lastPeriodIndex + 1:]\n","        endTime = currentSegment.get(\"end\")\n","\n","        newSegments = sentencesText.split(\".\")\n","        for newSegment in newSegments:\n","            if newSegment != \"\":\n","                newSegment = {\"text\": newSegment,\n","                            \"start\": thisStartTimeStamp, \"end\": endTime}\n","                newSegmentList.append(newSegment)\n","\n","        if reminderText is not None:\n","            transcribe(endTime, reminderText, segmentIndex + 1, segmentsArray)\n","        else:\n","            transcribe(None, None, segmentIndex + 1, segmentsArray)\n"],"metadata":{"id":"BMCVnL4v1YBV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##3. Upload any video files you want transcribed in the \"WhisperVideo\" folder in your Google Drive."],"metadata":{"id":"7fr8tBQy5Tvo"}},{"cell_type":"markdown","source":["##4. Extract audio from the video files and create a transcription"],"metadata":{"id":"nCef9V2i392e"}},{"cell_type":"code","source":["import json\n","\n","# an empty dictionary\n","newSegmentList = []\n","# Load all the audio file paths in a Google Drive folder\n","from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True) # This will prompt for authorization.\n","\n","# Get the list of video files from the WhisperVideo folder\n","video_files = os.listdir(\"/content/drive/MyDrive/WhisperVideo/\")\n","\n","# Loop through the video files and transcribe them\n","for video_file in video_files:\n","\n","  # Skip the file if it is not a video format\n","  if not video_file.endswith((\".mp4\", \".mov\", \".avi\", \".mkv\")):\n","    continue\n","\n","  # Extract the audio from the video file using librosa\n","  video_path = \"/content/drive/MyDrive/WhisperVideo/\" + video_file\n","  audio_path = \"/content/drive/MyDrive/WhisperVideo/AudioFiles/\" + video_file[:-4] + \".wav\" # Replace the video extension with .wav\n","\n","  y, sr = librosa.load(video_path, sr=16000) # Load the audio with 16 kHz sampling rate\n","  sf.write(audio_path, y, sr) # Save the audio as a wav file\n","\n","  # Transcribe the audio file using Whisper\n","  result = model.transcribe(audio_path)\n","\n","  print(json.dumps(result, indent = 2, ensure_ascii = False))\n","\n","  segments = result.get(\"segments\")\n","  print(json.dumps(segments, indent = 2, ensure_ascii = False))\n","  if segments is None:\n","      print(\"No segments found\")\n","      exit(1)\n","\n","  transcribe(None, None, 0, segments)\n","  \n","\n","  print(json.dumps(newSegmentList, indent = 2, ensure_ascii = False))\n","\n","  text = result[\"text\"].strip()\n","  text = text.replace(\". \", \".\\n\\n\")\n","\n","  # Save the transcription as a text file in Google Docs\n","  text_file = video_file[:-4] + \".txt\" # Replace the video extension with .txt\n","  text_path = \"/content/drive/MyDrive/WhisperVideo/TextFiles/\" + text_file\n","  with open(text_path, \"w\") as f:\n","    f.write(text)\n","\n","  json_file = video_file[:-4] + \".json\"\n","  json_path = \"/content/drive/MyDrive/WhisperVideo/JsonFiles/\" + json_file\n","  with open(json_path, \"w\") as f:\n","    f.write(json.dumps(newSegmentList, indent = 2, ensure_ascii = False))\n","    \n","  # Move the video file to the ProcessedVideo folder\n","  processed_path = \"/content/drive/MyDrive/WhisperVideo/ProcessedVideo/\" + video_file\n","  os.rename(video_path, processed_path)\n","\n","  # Print a message to indicate the progress\n","  print(\"Processed {video_file} and saved the transcription as {text_file}\")"],"metadata":{"id":"D_rB5M99nmhw"},"execution_count":null,"outputs":[]}]}